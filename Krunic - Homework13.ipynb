{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "# Initialize\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Question 13.1</h3>\n",
    "\n",
    "<p><i>Write a basic Spark implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input. Make sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iteration so that the output of each iteration is correctly normalized (sums to 1).</i></p>\n",
    "\n",
    "<h4>Solution</h4>\n",
    "\n",
    "<p>We import the PageRank test file and use that as our starting point. From there, a simple loop is computed over the data to complete the PageRank algorithm.</p>\n",
    "\n",
    "<p>References: \n",
    "<ul>\n",
    "<li><a href=\"https://www.youtube.com/watch?v=_Wc9OkMKS3g\">Sink Nodes in PageRank</a></li>\n",
    "<li><a href=\"https://github.com/apache/spark/blob/master/examples/src/main/python/pagerank.py\">PageRank in Spark</a> (Apache)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.37985079809520234, 'C'), (0.36031999461053826, u'B'), (0.0772920744806844, 'E'), (0.037443828260832486, 'F'), (0.037443828260832486, u'D')]\n"
     ]
    }
   ],
   "source": [
    "# Parse \n",
    "def parseRaw(line):\n",
    "    \n",
    "    \"\"\" This function parses the graph elements from the dictionary.\"\"\"\n",
    "    \n",
    "    # Separate\n",
    "    array = line.split('\\t')\n",
    "    node, neighbors = array \n",
    "    neighbors = eval(neighbors)\n",
    "    \n",
    "    # Emit\n",
    "    for k in neighbors.keys(): \n",
    "        yield (node, [k])\n",
    "        yield (k, [])\n",
    "        \n",
    "\n",
    "# Calculate PageRank \n",
    "def emitPR(line):\n",
    "    \n",
    "    # Unpack\n",
    "    node, prTuple = line\n",
    "    prList, rank = prTuple\n",
    "    \n",
    "    # Emit \n",
    "    for neighbor in prList: \n",
    "        yield (neighbor, rank / len(prList))\n",
    "        \n",
    "        # Danglers \n",
    "        yield (node, 0)\n",
    "        \n",
    "        \n",
    "# Compute PR with Teleport \n",
    "def dampenedPR(line, d=0.85):\n",
    "        \n",
    "    # Unpack\n",
    "    node, PR = line \n",
    "    \n",
    "    # Update \n",
    "    PR *= d\n",
    "    PR += (1 - d + d * danglerLoss / totalDanglers ) / totalNodes\n",
    "    \n",
    "    # Emit \n",
    "    return (node, PR)    \n",
    "        \n",
    "        \n",
    "######## INITIALIZE #########\n",
    "\n",
    "# Load \n",
    "graphData = sc.textFile('./pagerank_test.txt').flatMap(lambda x: parseRaw(x)).reduceByKey(lambda a, b: a + b)\n",
    "rank = graphData.map(lambda x: (x[0], 1))\n",
    "\n",
    "# Dangling \n",
    "danglers = graphData.filter(lambda x: not bool(x[1]))\n",
    "totalDanglers = danglers.count()\n",
    "\n",
    "# Diag\n",
    "totalNodes = graphData.count()\n",
    "\n",
    "# PageRank\n",
    "for i in range(10):\n",
    "\n",
    "    # Neighbor contributions \n",
    "    PR = graphData.join(rank).flatMap(lambda x: emitPR(x)).reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # Dangling PR\n",
    "    danglerPR = PR.join(danglers).map(lambda x: (x[0], x[1][0]))\n",
    "    danglerLoss = danglerPR.map(lambda x: x[1]).sum()\n",
    "\n",
    "    # Dampening\n",
    "    dPR = PR.map(lambda x: dampenedPR(x))\n",
    "\n",
    "    # Normalize \n",
    "    totalWeight = dPR.map(lambda x: x[1]).sum()\n",
    "    nPR = dPR.map(lambda x: (x[0], x[1] / totalWeight))\n",
    "\n",
    "    # Cycle \n",
    "    rank = nPR\n",
    "\n",
    "# Results \n",
    "print rank.map(lambda x: (x[1], x[0])).sortByKey(ascending=False).take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Question 13.2</h3>\n",
    "\n",
    "<p><i>Run your Spark PageRank implementation on the Wikipedia dataset for 10 iterations,\n",
    "and display the top 100 ranked nodes (with alpha = 0.85).</i></p>\n",
    "\n",
    "<h4>Solution</h4>\n",
    "\n",
    "<p>We write our Python file with the necessary adjustments to point to the S3 bucket. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sparkEMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sparkEMR.py\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "# Initialize\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# Parse \n",
    "def parseRaw(line):\n",
    "    \n",
    "    \"\"\" This function parses the graph elements from the dictionary.\"\"\"\n",
    "    \n",
    "    # Separate\n",
    "    array = line.split('\\t')\n",
    "    node, neighbors = array \n",
    "    neighbors = eval(neighbors)\n",
    "    \n",
    "    # Emit\n",
    "    for k in neighbors.keys(): \n",
    "        yield (node, [k])\n",
    "        yield (k, [])\n",
    "        \n",
    "\n",
    "# Calculate PageRank \n",
    "def emitPR(line):\n",
    "    \n",
    "    # Unpack\n",
    "    node, prTuple = line\n",
    "    prList, rank = prTuple\n",
    "    \n",
    "    # Emit \n",
    "    for neighbor in prList: \n",
    "        yield (neighbor, rank / len(prList))\n",
    "        \n",
    "        # Danglers \n",
    "        yield (node, 0)\n",
    "        \n",
    "        \n",
    "# Compute PR with Teleport \n",
    "def dampenedPR(line, d=0.85):\n",
    "        \n",
    "    # Unpack\n",
    "    node, PR = line \n",
    "    \n",
    "    # Update \n",
    "    PR *= d\n",
    "    PR += (1 - d + d * danglerLoss / totalDanglers ) / totalNodes\n",
    "    \n",
    "    # Emit \n",
    "    return (node, PR)    \n",
    "        \n",
    "        \n",
    "######## INITIALIZE #########\n",
    "\n",
    "\n",
    "dataFile = (\"s3n://AKIAJQOD4KMA46R45NCA:\"\n",
    "    \"ASonGZ4q98UmcQInjZHXEy8VWbgc/E5BojK9UwuE\"\n",
    "    \"@s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt\")\n",
    "\n",
    "# Load \n",
    "graphData = sc.textFile(dataFile).flatMap(lambda x: parseRaw(x)).reduceByKey(lambda a, b: a + b)\n",
    "rank = graphData.map(lambda x: (x[0], 1))\n",
    "\n",
    "# Dangling \n",
    "danglers = graphData.filter(lambda x: not bool(x[1]))\n",
    "totalDanglers = danglers.count()\n",
    "\n",
    "# Diag\n",
    "totalNodes = graphData.count()\n",
    "\n",
    "# PageRank\n",
    "for i in range(10):\n",
    "\n",
    "    # Neighbor contributions \n",
    "    PR = graphData.join(rank).flatMap(lambda x: emitPR(x)).reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # Dangling PR\n",
    "    danglerPR = PR.join(danglers).map(lambda x: (x[0], x[1][0]))\n",
    "    danglerLoss = danglerPR.map(lambda x: x[1]).sum()\n",
    "\n",
    "    # Dampening\n",
    "    dPR = PR.map(lambda x: dampenedPR(x))\n",
    "\n",
    "    # Normalize \n",
    "    totalWeight = dPR.map(lambda x: x[1]).sum()\n",
    "    nPR = dPR.map(lambda x: (x[0], x[1] / totalWeight))\n",
    "\n",
    "    # Cycle \n",
    "    rank = nPR\n",
    "\n",
    "# Results \n",
    "print rank.map(lambda x: (x[1], x[0])).sortByKey(ascending=False).take(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
